{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_parquet(df_path)\n",
    "from functions_training_pipeline import import_data, data_normalization\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = r\"../Data/combined/dataframe_extended/\"\n",
    "date_from = '2018-06-13'\n",
    "date_to = '2018-06-17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = import_data(date_from, date_to, df_path)\n",
    "data = data_normalization(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_training_pipeline_pyspark as f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/05/13 12:44:46 WARN Utils: Your hostname, LAPTOP-3TD5GQG7 resolves to a loopback address: 127.0.1.1; using 172.20.146.42 instead (on interface eth0)\n",
      "23/05/13 12:44:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/13 12:44:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"LogisticRegressionGridSearch\").getOrCreate()\n",
    "\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = f.Model(model=LinearRegression, name=\"LinearRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_for_grid = {\"fit_intercept\": [True]}\n",
    "lr.hyperparameters = lr.create_hyperparameter_grid(hyperparameters_for_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = data.columns.drop([\"opt_value\"])\n",
    "data_ = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/13 13:35:48 ERROR Instrumentation: java.lang.IllegalArgumentException: features does not exist. Available: x, y\n",
      "\tat org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:285)\n",
      "\tat scala.collection.immutable.Map$Map2.getOrElse(Map.scala:236)\n",
      "\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:284)\n",
      "\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnTypes(SchemaUtils.scala:59)\n",
      "\tat org.apache.spark.ml.util.SchemaUtils$.validateVectorCompatibleColumn(SchemaUtils.scala:205)\n",
      "\tat org.apache.spark.ml.clustering.KMeansParams.validateAndTransformSchema(KMeans.scala:121)\n",
      "\tat org.apache.spark.ml.clustering.KMeansParams.validateAndTransformSchema$(KMeans.scala:120)\n",
      "\tat org.apache.spark.ml.clustering.KMeans.validateAndTransformSchema(KMeans.scala:295)\n",
      "\tat org.apache.spark.ml.clustering.KMeans.transformSchema(KMeans.scala:599)\n",
      "\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.KMeans.$anonfun$fit$1(KMeans.scala:372)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:371)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: x, y",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/AWS_Scripts/temp_spark.ipynb Cell 8\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/AWS_Scripts/temp_spark.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m lr\u001b[39m.\u001b[39mspatial_cv(data_, columns, target_normalized\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/AWS_Scripts/functions_training_pipeline_pyspark.py:105\u001b[0m, in \u001b[0;36mModel.spatial_cv\u001b[0;34m(self, df, columns, target_normalized)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv_model_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    104\u001b[0m \u001b[39m# Split the data into outer folds\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__kmeans_split(df, \u001b[39m\"\u001b[39;49m\u001b[39mouter_area\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    107\u001b[0m \u001b[39m# Create a SparkSession\u001b[39;00m\n\u001b[1;32m    108\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mgetOrCreate()\n",
      "File \u001b[0;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/AWS_Scripts/functions_training_pipeline_pyspark.py:30\u001b[0m, in \u001b[0;36mModel.__kmeans_split\u001b[0;34m(self, df, split_variable_name, plot, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__kmeans_split\u001b[39m(\u001b[39mself\u001b[39m, df, split_variable_name, plot\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 30\u001b[0m     kmeans \u001b[39m=\u001b[39m KMeans(k\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, seed\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(df\u001b[39m.\u001b[39;49mselect(\u001b[39m\"\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     31\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwithColumn(split_variable_name, kmeans\u001b[39m.\u001b[39mtransform(df\u001b[39m.\u001b[39mselect(\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39mselect(\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39m# not sure about select prediction\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[39mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/.venv/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/.venv/lib/python3.9/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/.venv/lib/python3.9/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/.venv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/.venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: features does not exist. Available: x, y"
     ]
    }
   ],
   "source": [
    "\n",
    "lr.spatial_cv(data_, columns, target_normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, model, name):\n",
    "        self.model = model\n",
    "        self.hyperparameters = []  # list of dictionaries with hyperparameters\n",
    "        self.name = name\n",
    "\n",
    "    def create_hyperparameter_grid(self, hyperparameters):\n",
    "        # # create a ParamGridBuilder object\n",
    "        # param_grid = ParamGridBuilder()\n",
    "        # # add hyperparameters and their possible values to the grid\n",
    "        # for hyperparam, values in hyperparameters.items():\n",
    "        #     param_grid = param_grid.addGrid(hyperparam, values)\n",
    "        # # build the grid of hyperparameters\n",
    "        # hyperparameters_grid = param_grid.build()\n",
    "        # return hyperparameters_grid\n",
    "        return ParamGridBuilder().build()\n",
    "\n",
    "    def __kmeans_split(self, df, split_variable_name, plot=False, verbose=False):\n",
    "        kmeans = KMeans(k=5, seed=0).fit(df.select(\"x\", \"y\"))\n",
    "        df = df.withColumn(split_variable_name, kmeans.transform(df.select(\"x\", \"y\")).select(\"prediction\")) # not sure about select prediction\n",
    "\n",
    "        if verbose:\n",
    "            df.groupBy(split_variable_name).count().show()\n",
    "\n",
    "        if plot:\n",
    "            # Convert Spark DataFrame to Pandas DataFrame for plotting\n",
    "            pandas_df = df.select(\"x\", \"y\", split_variable_name).toPandas()\n",
    "            plt.scatter(pandas_df[\"x\"], pandas_df[\"y\"], c=pandas_df[split_variable_name], edgecolor=\"none\", s=0.05)\n",
    "            plt.show()\n",
    "        return df\n",
    "\n",
    "    def __train_test_split(self, df, columns, split_variable_name, split_index):\n",
    "        train = df.filter(df[split_variable_name] != split_index)\n",
    "        test = df.filter(df[split_variable_name] == split_index)\n",
    "        train_X = train.select(columns)\n",
    "        train_y = train.select(\"opt_value\").rdd.flatMap(lambda x: x).collect()\n",
    "        test_X = test.select(columns)\n",
    "        test_y = test.select(\"opt_value\").rdd.flatMap(lambda x: x).collect()\n",
    "        return train_X, train_y, test_X, test_y\n",
    "\n",
    "    def __tune_hyperparameters(self, df, columns, split_variable_name):\n",
    "        \"\"\"\n",
    "        This function performs hyperparameter tuning in the inner loop of nested cross-validation.\n",
    "\n",
    "        Args:\n",
    "            df (pyspark.sql.DataFrame): DataFrame with data.\n",
    "\n",
    "            columns (list): List with column names to be used in the model.\n",
    "\n",
    "            split_variable_name (str): Name of column with k-means split.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary with best hyperparameters.\n",
    "        \"\"\"\n",
    "        all_hyperparameter_scores = []\n",
    "        for split in df.select(split_variable_name).distinct().rdd.flatMap(lambda x: x).collect():\n",
    "            train_X, train_y, test_X, test_y = self.__train_test_split(df, columns, split_variable_name, split)\n",
    "            one_loop_hyperparameter_scores = []\n",
    "            if isinstance(self.hyperparameters, list):\n",
    "                for hyperparams in self.hyperparameters:\n",
    "                    regressor = self.model(**hyperparams)\n",
    "                    evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "                    param_grid = ParamGridBuilder().build()\n",
    "                    crossval = CrossValidator(estimator=regressor, estimatorParamMaps=param_grid, evaluator=evaluator)\n",
    "                    cv_model = crossval.fit(train_X)\n",
    "                    predictions = cv_model.transform(test_X)\n",
    "                    rmse = evaluator.evaluate(predictions)\n",
    "                    one_loop_hyperparameter_scores.append(rmse)\n",
    "            else:\n",
    "                print(\"hyperparameters must be a list\")\n",
    "            all_hyperparameter_scores.append(one_loop_hyperparameter_scores)\n",
    "\n",
    "        mean_hyperparameters = np.mean(all_hyperparameter_scores, axis=0)\n",
    "        best_hyperparameters = self.hyperparameters[np.argmin(mean_hyperparameters)]\n",
    "        return best_hyperparameters\n",
    "\n",
    "    def __check_columns(self, columns):\n",
    "        for col in columns:\n",
    "            if col in [\"row\", \"col\", \"date\", \"opt_value\"]:\n",
    "                print(f\"Column {col} should not be included\")\n",
    "                assert False\n",
    "\n",
    "    def spatial_cv(self, df, columns, target_normalized):\n",
    "        self.__check_columns(columns)\n",
    "        self.columns = columns\n",
    "\n",
    "        rmse_list_train = []\n",
    "        rmse_list_test = []\n",
    "        r2_list_train = []\n",
    "        r2_list_test = []\n",
    "        self.cv_model_list = []\n",
    "\n",
    "        # Split the data into outer folds\n",
    "        df = self.__kmeans_split(df, \"outer_area\")\n",
    "\n",
    "        # Create a SparkSession\n",
    "        #spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "        # df_spark = spark.createDataFrame(df)\n",
    "\n",
    "        # For each outer fold\n",
    "        for outer_split in df.select(\"outer_area\").distinct().rdd.flatMap(lambda x: x).collect():\n",
    "            print(\"Spatial CV, outer split:\", outer_split)\n",
    "\n",
    "            # Define only train set (to be used in the inner loop of nested cross-validation)\n",
    "            train = df.filter(df[\"outer_area\"] != outer_split)\n",
    "\n",
    "            # Split the data into inner folds\n",
    "            train = self.__kmeans_split(train, \"inner_area\")\n",
    "\n",
    "            # Tune hyperparameters (all inner loops of nested cross-validation are executed in this function)\n",
    "            best_hyperparam = self.__tune_hyperparameters(train, columns, split_variable_name=\"inner_area\")\n",
    "\n",
    "            # With the best hyperparameters, train the model on the outer fold\n",
    "            train_X, train_y, test_X, test_y = self.__train_test_split(\n",
    "                df, columns, split_variable_name=\"outer_area\", split_index=outer_split\n",
    "            )\n",
    "\n",
    "            # Convert train and test data to Spark DataFrame\n",
    "            train_data = spark.createDataFrame(train_X.rdd.zipWithIndex().map(lambda x: (x[1],) + x[0]).toDF())\n",
    "            train_data = train_data.toDF(*[\"index\"] + train_X.columns)\n",
    "            train_data = train_data.withColumn(\"opt_value\", lit(train_y))\n",
    "\n",
    "            test_data = spark.createDataFrame(test_X.rdd.zipWithIndex().map(lambda x: (x[1],) + x[0]).toDF())\n",
    "            test_data = test_data.toDF(*[\"index\"] + test_X.columns)\n",
    "            test_data = test_data.withColumn(\"opt_value\", lit(test_y))\n",
    "\n",
    "            # Create the model with the best hyperparameters\n",
    "            regressor = self.model(**best_hyperparam)\n",
    "            model = regressor.fit(train_data)\n",
    "            self.cv_model_list.append(model)\n",
    "\n",
    "            # Make predictions on train and test data\n",
    "            train_y_predicted = model.transform(train_data).select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "            test_y_predicted = model.transform(test_data).select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "            if target_normalized:\n",
    "                train_y_predicted = [np.exp(y) - 1 for y in train_y_predicted]\n",
    "                test_y_predicted = [np.exp(y) - 1 for y in test_y_predicted]\n",
    "\n",
    "            evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "            rmse_list_train = evaluator.evaluate(train_y_predicted)\n",
    "            rmse_list_test = evaluator.evaluate(test_y_predicted)\n",
    "            evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "            r2_list_train = evaluator.evaluate(train_y_predicted)\n",
    "            r2_list_test = evaluator.evaluate(test_y_predicted)\n",
    "\n",
    "        # Results\n",
    "        self.rmse_train = np.mean(rmse_list_train)\n",
    "        self.rmse_std_train = np.std(rmse_list_train)\n",
    "        self.rmse_test = np.mean(rmse_list_test)\n",
    "        self.rmse_std_test = np.std(rmse_list_test)\n",
    "        self.r2_train = np.mean(r2_list_train)\n",
    "        self.r2_std_train = np.std(r2_list_train)\n",
    "        self.r2_test = np.mean(r2_list_test)\n",
    "        self.r2_std_test = np.std(r2_list_test)\n",
    "\n",
    "        # Find best hyperparameters for the WHOLE dataset (instead of only one fold at a time)\n",
    "        df = self.__kmeans_split(df, \"final_split_areas\")\n",
    "        for split in df.select(\"final_split_areas\").distinct().rdd.flatMap(lambda x: x).collect():\n",
    "            print(\"Spatial CV, final split:\", split)\n",
    "            final_hyperparameters = self.__tune_hyperparameters(df, columns, split_variable_name=\"final_split_areas\")\n",
    "\n",
    "        # Fit final model\n",
    "        self.final_model = self.model(**final_hyperparameters)\n",
    "        final_data = spark.createDataFrame(df.rdd.zipWithIndex().map(lambda x: (x[1],) + x[0]).toDF())\n",
    "        final_data = final_data.toDF(*[\"index\"] + df.columns)\n",
    "        final_data = final_data.withColumn(\"opt_value\", lit(df[\"opt_value\"]))\n",
    "        self.final_model = self.final_model.fit(final_data)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
