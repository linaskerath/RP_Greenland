{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose of script:**\n",
    "\n",
    "Creating new features \n",
    "\n",
    "- In: dataframe_plain\n",
    "- Out: dataframe_extended (with additional feature columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "import rasterio\n",
    "import gemgis as gg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Relevant paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataframe_plain = r\"../Data/combined/dataframe_plain/\"\n",
    "mw_path = r\"../Data/microwave-rs/mw_interpolated/\"\n",
    "path_elevation =  r\"../Data/elevation_data/gimpdem_1km_compressed.tif\"\n",
    "out_path = r\"../Data/combined/dataframe_extended/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Row and column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row_and_col(df):\n",
    "        # add row and column features:\n",
    "        df['col'] = df.groupby(\"x\").ngroup() # xshape 2663 \n",
    "        df['row'] = df.groupby(\"y\").ngroup(ascending=False) # yshape 1462\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date(df):\n",
    "    df['date'] = '2019-07-01'\n",
    "    df[\"date\"]= pd.to_datetime(df[\"date\"])\n",
    "    # df['month'] = df[\"date\"].dt.month\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggregated/pooled values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_neighbors(matrix, a, b):\n",
    "#     neighbors = [matrix[i][j] if (i > -1 and j > -1 and j < len(matrix[0]) and i < len(matrix)) else np.nan for i in range(a-1, a+2) for j in range(b-1, b+2) ]\n",
    "#     return neighbors\n",
    "\n",
    "# def get_neighbours_df(data):\n",
    "#     index_list = [(i,j) for i in range(data.shape[1]) for j in range(data.shape[2])]\n",
    "#     value_list = []\n",
    "#     values = data.values[0]\n",
    "\n",
    "#     for i in tqdm(index_list):\n",
    "#         neighbor = get_neighbors(values, *i)\n",
    "#         neighbor += [i[0], i[1]]\n",
    "#         value_list.append(neighbor)\n",
    "\n",
    "#     df_neighbors = pd.DataFrame(value_list, columns = ['v1', 'v2', 'v3','v4', 'v5', 'v6','v7', 'v8', 'v9', 'row', 'col'])   \n",
    "#     return df_neighbors\n",
    "\n",
    "# def add_aggregated(df):\n",
    "#     cols = ['v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9'] # delet v5? same as mw value\n",
    "#     df[cols] = df[cols].replace(-1, np.NaN) # to skip -1 when calculating mean\n",
    "#     df['mean'] = df[cols].mean(axis = 1) # mean value of 9 pixels around\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from typing import Tuple\n",
    "from scipy.stats import mode\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "def get_window(image: np.ndarray, window_size: int, center: Tuple[int, int]) -> np.ndarray:\n",
    "    top = max(center[0] - window_size // 2, 0)\n",
    "    bottom = min(center[0] + window_size // 2 + 1, image.shape[0])\n",
    "    left = max(center[1] - window_size // 2, 0)\n",
    "    right = min(center[1] + window_size // 2 + 1, image.shape[1])\n",
    "    window = image[top:bottom, left:right]\n",
    "    return window\n",
    "\n",
    "\n",
    "# need to fix? : only calculate if the middle value is not nan - else all nan columns around 1 and 0 are going to have a value.\n",
    "# -1 to nan\n",
    "\n",
    "def convolve(image, window_size, convolution_fn: Union['mean', 'min', 'max', 'sum', 'median', 'mode']):\n",
    "    image = image[0].values\n",
    "    image[image == -1] = np.nan\n",
    "    \n",
    "    if convolution_fn == 'mean':\n",
    "        kernel = np.ones((window_size, window_size))  # kernel for mean convolution\n",
    "        result = np.zeros_like(image, dtype=np.float64)\n",
    "        # Compute the sum and count of non-NaN values in the kernel window\n",
    "        counts = convolve2d(~np.isnan(image), kernel, mode='same', boundary='fill', fillvalue=0)\n",
    "        sums = convolve2d(np.nan_to_num(image), kernel, mode='same', boundary='fill', fillvalue=0)\n",
    "        # Calculate the mean, ignoring NaN values\n",
    "        result[counts > 0] = sums[counts > 0] / counts[counts > 0]\n",
    "        # Set the output to NaN where all values in the kernel window are NaN\n",
    "        result[counts == 0] = np.nan\n",
    "        return result\n",
    "    elif convolution_fn == 'max':\n",
    "        result = np.zeros_like(image, dtype=np.float64)\n",
    "        for i in range(image.shape[0]):\n",
    "            for j in range(image.shape[1]):\n",
    "                window = get_window(image, window_size, (i, j))\n",
    "                non_nan_values = window[~np.isnan(window)]\n",
    "                if len(non_nan_values) == 0:\n",
    "                    result[i, j] = np.nan\n",
    "                else:\n",
    "                    result[i, j] = np.nanmax(non_nan_values)\n",
    "        return result\n",
    "\n",
    "    elif convolution_fn == 'min':\n",
    "        result = np.zeros_like(image, dtype=np.float64)\n",
    "        for i in range(image.shape[0]):\n",
    "            for j in range(image.shape[1]):\n",
    "                window = get_window(image, window_size, (i, j))\n",
    "                non_nan_values = window[~np.isnan(window)]\n",
    "                if len(non_nan_values) == 0:\n",
    "                    result[i, j] = np.nan\n",
    "                else:\n",
    "                    result[i, j] = np.nanmin(non_nan_values)\n",
    "        return result\n",
    "\n",
    "    elif convolution_fn == 'sum':\n",
    "        result = np.zeros_like(image, dtype=np.float64)\n",
    "        for i in range(image.shape[0]):\n",
    "            for j in range(image.shape[1]):\n",
    "                window = get_window(image, window_size, (i, j))\n",
    "                non_nan_values = window[~np.isnan(window)]\n",
    "                if len(non_nan_values) == 0:\n",
    "                    result[i, j] = np.nan\n",
    "                else:\n",
    "                    result[i, j] = np.nansum(non_nan_values)\n",
    "        return result\n",
    "        \n",
    "    elif convolution_fn == 'median':\n",
    "        result = np.zeros_like(image, dtype=np.float64)\n",
    "        for i in range(image.shape[0]):\n",
    "            for j in range(image.shape[1]):\n",
    "                window = get_window(image, window_size, (i, j))\n",
    "                non_nan_values = window[~np.isnan(window)]\n",
    "                if len(non_nan_values) == 0:\n",
    "                    result[i, j] = np.nan\n",
    "                else:\n",
    "                    result[i, j] = np.nanmedian(non_nan_values)\n",
    "        return result\n",
    "    elif convolution_fn == 'mode':\n",
    "        result = np.zeros_like(image, dtype=np.float64)\n",
    "        for i in range(image.shape[0]):\n",
    "            for j in range(image.shape[1]):\n",
    "                window = get_window(image, window_size, (i, j))\n",
    "                non_nan_values = window[~np.isnan(window)]\n",
    "                if len(non_nan_values) == 0:\n",
    "                    result[i, j] = np.nan\n",
    "                else:\n",
    "                    result[i, j] = mode(non_nan_values)[0]\n",
    "        return result\n",
    "    else: \n",
    "        print('not available function')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_to_df(convolution_raster, column = Union['mean', 'min', 'max', 'sum', 'median', 'mode']):\n",
    "    nrows, ncols = convolution_raster.shape\n",
    "    # create an array of x and y positions\n",
    "    x = np.tile(np.arange(ncols), nrows)\n",
    "    y = np.repeat(np.arange(nrows), ncols)\n",
    "    # create a DataFrame with x, y, and pixel values as columns\n",
    "    df = pd.DataFrame({'col': x, 'row': y, column: convolution_raster.flatten()})\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Elevation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elevation(data):\n",
    "    df = data.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "    df = df[['x', 'y', 'band_data']]\n",
    "    df.rename({'band_data': 'elevation_data'}, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope(data):\n",
    "    slope = gg.raster.calculate_slope(data)\n",
    "    nrows, ncols = slope.shape\n",
    "    # create an array of x and y positions\n",
    "    x = np.tile(np.arange(ncols), nrows)\n",
    "    y = np.repeat(np.arange(nrows), ncols)\n",
    "    # create a DataFrame with x, y, and pixel values as columns\n",
    "    df_slope = pd.DataFrame({'col': x, 'row': y, 'slope_data': slope.flatten()})\n",
    "    return df_slope"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect(data):\n",
    "    aspect = gg.raster.calculate_aspect(data)\n",
    "    nrows, ncols = aspect.shape\n",
    "    # create an array of x and y positions\n",
    "    x = np.tile(np.arange(ncols), nrows)\n",
    "    y = np.repeat(np.arange(nrows), ncols)\n",
    "    # create a DataFrame with x, y, and pixel values as columns\n",
    "    df_aspect = pd.DataFrame({'col': x, 'row': y, 'aspect_data': aspect.flatten()})\n",
    "    return df_aspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance from margin/shore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbd\n",
    "# add if coast column - if at least one na but not all "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(mw_path, path_dataframe_plain, path_elevation, out_path):\n",
    "    # get plain files:\n",
    "    df_plain_files = [f for f in listdir(path_dataframe_plain) if isfile(join(path_dataframe_plain, f))]\n",
    "    # microwave files:\n",
    "    mw_files = [f for f in listdir(mw_path) if isfile(join(mw_path, f))]\n",
    "    # elevation file as xarray and rasterio:\n",
    "    data_elevation_xarray = xarray.open_dataarray(path_elevation)\n",
    "    data_elevation_rasterio = rasterio.open(path_elevation)\n",
    "\n",
    "\n",
    "    for df_file in df_plain_files:\n",
    "        melt_date =  df_file[5:15]\n",
    "        print(melt_date)\n",
    "        for mw_file in mw_files:\n",
    "            if mw_file.startswith(melt_date):\n",
    "                data_mw = xarray.open_dataarray(mw_path + mw_file)\n",
    "                df = pd.read_parquet(path_dataframe_plain + df_file)\n",
    "                # add row and column features:\n",
    "                df = add_row_and_col(df)\n",
    "                # get convolution:\n",
    "                convolution_function = 'mean'\n",
    "                convolution = convolve(data_mw, 3, convolution_function)\n",
    "                # convolution to df:\n",
    "                df_conv = convolution_to_df(convolution, convolution_function)\n",
    "                # merge convolution:\n",
    "                df_combined = pd.merge(df, df_conv, how = 'left', on = ['row', 'col'])\n",
    "                # remove water in mw:\n",
    "                df_combined = df_combined.loc[df_combined['mw_value'] != -1] # suppress warning?\n",
    "                # add date:\n",
    "                df = add_date(df_combined)\n",
    "                # add elevation data:\n",
    "                df_elevation = add_elevation(data_elevation_xarray)\n",
    "                # get slope data:\n",
    "                df_slope = get_slope(data_elevation_rasterio)\n",
    "                # merge slope data:\n",
    "                df_elevation = pd.merge(df_elevation, df_slope[[\"slope_data\"]], how=\"left\", right_index=True, left_index=True)\n",
    "                # get aspect data:\n",
    "                df_aspect = get_aspect(data_elevation_rasterio)\n",
    "                # merge aspect data:\n",
    "                df_elevation = pd.merge(df_elevation, df_aspect[[\"aspect_data\"]], how=\"left\", right_index=True, left_index=True)\n",
    "                # merge elevation data:\n",
    "                df_with_elevation = pd.merge(df, df_elevation, how = 'left', on = ['y', 'x']) # left smaller mw, right - opt               \n",
    "                # write to parquet:\n",
    "                df_with_elevation.to_parquet(out_path + 'melt_'+ melt_date + '_extended.parquet.gzip', index= False)                    \n",
    "    return\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3893306/3893306 [00:33<00:00, 117824.02it/s]\n"
     ]
    }
   ],
   "source": [
    "main(mw_path, path_dataframe_plain, path_elevation, out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Lina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt_date =  '2019-06-08'\n",
    "#data_elevation_xarray = xarray.open_dataarray(path_elevation)\n",
    "data_mw = xarray.open_dataarray(mw_path + '2019-06-08_mw.tif')\n",
    "df = pd.read_parquet(path_dataframe_plain + 'melt_2019-06-08.parquet.gzip')\n",
    "\n",
    "# add row and column features:\n",
    "df = add_row_and_col(df)\n",
    "\n",
    "test1 = convolve(data_mw, 3, 'mean')\n",
    "df_conv = convolution_to_df(test1, 'mean')\n",
    "df_combined = pd.merge(df, df_conv, how = 'left', on = ['row', 'col'])\n",
    "# # merge neighbours:\n",
    "# df_combined = pd.merge(df, df_neighbors, how = 'left', on = ['row', 'col'])\n",
    "# # remove water in mw:\n",
    "# df_combined = df_combined.loc[df_combined['mw_value'] != -1] # suppress warning?\n",
    "# # add date:\n",
    "# df = add_date(df_combined)\n",
    "# # add aggregations:\n",
    "# df = add_aggregated(df)\n",
    "\n",
    "\n",
    "# # add elevation data:\n",
    "# df_elevation = add_elevation(data_elevation_xarray)\n",
    "# # merge elevation data:\n",
    "# df_with_elevation = pd.merge(df, df_elevation, how = 'left', on = ['y', 'x']) # left smaller mw, right - opt\n",
    "                \n",
    "# # write to parquet:\n",
    "# df_with_elevation.to_parquet(out_path + 'melt_'+ melt_date + '_extended.parquet.gzip', index= False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>mw_value</th>\n",
       "      <th>opt_value</th>\n",
       "      <th>col</th>\n",
       "      <th>row</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-636500.0</td>\n",
       "      <td>-662500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-635500.0</td>\n",
       "      <td>-662500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-634500.0</td>\n",
       "      <td>-662500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-633500.0</td>\n",
       "      <td>-662500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-632500.0</td>\n",
       "      <td>-662500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3893301</th>\n",
       "      <td>820500.0</td>\n",
       "      <td>-3324500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1457</td>\n",
       "      <td>2662</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3893302</th>\n",
       "      <td>821500.0</td>\n",
       "      <td>-3324500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1458</td>\n",
       "      <td>2662</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3893303</th>\n",
       "      <td>822500.0</td>\n",
       "      <td>-3324500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1459</td>\n",
       "      <td>2662</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3893304</th>\n",
       "      <td>823500.0</td>\n",
       "      <td>-3324500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1460</td>\n",
       "      <td>2662</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3893305</th>\n",
       "      <td>824500.0</td>\n",
       "      <td>-3324500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1461</td>\n",
       "      <td>2662</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3893306 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                x          y  mw_value  opt_value   col   row  mean\n",
       "0       -636500.0  -662500.0       0.0       -1.0     0     0   0.0\n",
       "1       -635500.0  -662500.0       0.0       -1.0     1     0   0.0\n",
       "2       -634500.0  -662500.0       0.0       -1.0     2     0   0.0\n",
       "3       -633500.0  -662500.0       0.0       -1.0     3     0   0.0\n",
       "4       -632500.0  -662500.0       0.0       -1.0     4     0   0.0\n",
       "...           ...        ...       ...        ...   ...   ...   ...\n",
       "3893301  820500.0 -3324500.0      -1.0       -1.0  1457  2662   NaN\n",
       "3893302  821500.0 -3324500.0      -1.0       -1.0  1458  2662   NaN\n",
       "3893303  822500.0 -3324500.0      -1.0       -1.0  1459  2662   NaN\n",
       "3893304  823500.0 -3324500.0      -1.0       -1.0  1460  2662   NaN\n",
       "3893305  824500.0 -3324500.0      -1.0       -1.0  1461  2662   NaN\n",
       "\n",
       "[3893306 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing values around the 0-1 change in the data:\n",
    "\n",
    "tt = data_mw.values\n",
    "# indices = np.where(tt == 1)\n",
    "tt[0][74:80, 622:628]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Nina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_elevation_xarray = xarray.open_dataarray(path_elevation)\n",
    "data_elevation_rasterio = rasterio.open(path_elevation)\n",
    "# add elevation data:\n",
    "df_elevation = add_elevation(data_elevation_xarray)\n",
    "# get slope data:\n",
    "df_slope = get_slope(data_elevation_rasterio)\n",
    "# merge slope data:\n",
    "df_elevation = pd.merge(df_elevation, df_slope[[\"slope_data\"]], how=\"left\", right_index=True, left_index=True)\n",
    "# get aspect data:\n",
    "df_aspect = get_aspect(data_elevation_rasterio)\n",
    "# merge aspect data:\n",
    "df_elevation = pd.merge(df_elevation, df_aspect[[\"aspect_data\"]], how=\"left\", right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>elevation_data</th>\n",
       "      <th>slope_data</th>\n",
       "      <th>aspect_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3995569.00</td>\n",
       "      <td>3995569.00</td>\n",
       "      <td>3995569.00</td>\n",
       "      <td>3995569.00</td>\n",
       "      <td>3995569.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>106500.00</td>\n",
       "      <td>-2005500.00</td>\n",
       "      <td>988.54</td>\n",
       "      <td>1.19</td>\n",
       "      <td>111.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>429259.88</td>\n",
       "      <td>775670.13</td>\n",
       "      <td>1122.30</td>\n",
       "      <td>3.26</td>\n",
       "      <td>124.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-636500.00</td>\n",
       "      <td>-3348500.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-265500.00</td>\n",
       "      <td>-2677500.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>106500.00</td>\n",
       "      <td>-2005500.00</td>\n",
       "      <td>306.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>50.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>478500.00</td>\n",
       "      <td>-1333500.00</td>\n",
       "      <td>2102.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>243.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>849500.00</td>\n",
       "      <td>-662500.00</td>\n",
       "      <td>3499.00</td>\n",
       "      <td>50.83</td>\n",
       "      <td>359.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               x           y  elevation_data  slope_data  aspect_data\n",
       "count 3995569.00  3995569.00      3995569.00  3995569.00   3995569.00\n",
       "mean   106500.00 -2005500.00          988.54        1.19       111.48\n",
       "std    429259.88   775670.13         1122.30        3.26       124.35\n",
       "min   -636500.00 -3348500.00            0.00        0.00         0.00\n",
       "25%   -265500.00 -2677500.00           35.00        0.00         0.00\n",
       "50%    106500.00 -2005500.00          306.00        0.10        50.06\n",
       "75%    478500.00 -1333500.00         2102.00        0.50       243.43\n",
       "max    849500.00  -662500.00         3499.00       50.83       359.95"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_elevation.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe81b6f13d6b0ad05b54a8d717c7eaa3743b1b1bf0e5cb2d6ec80103feaa1c84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
