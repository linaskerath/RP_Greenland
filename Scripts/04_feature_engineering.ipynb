{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose of script:**\n",
    "\n",
    "Creating new features \n",
    "\n",
    "- In: dataframe_plain\n",
    "- Out: dataframe_extended (with additional feature columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "import rasterio\n",
    "import gemgis as gg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from astral.sun import sun\n",
    "from astral import LocationInfo\n",
    "import pyproj\n",
    "from math import asin, cos, atan, tan, pi, acos, sin \n",
    "import os\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Relevant paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataframe_plain = r\"../Data/combined/dataframe_plain/\"\n",
    "mw_path = r\"../Data/microwave-rs/mw_interpolated/\"\n",
    "path_elevation =  r\"../Data/elevation_data/gimpdem_1km_compressed.tif\"\n",
    "path_daylight = r\"../Data/daylight_data/\"\n",
    "out_path = r\"../Data/combined/dataframe_extended/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Row and column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row_and_col(df):\n",
    "        # add row and column features:\n",
    "        df['col'] = df.groupby(\"x\").ngroup() # xshape 2663 \n",
    "        df['row'] = df.groupby(\"y\").ngroup(ascending=False) # yshape 1462\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date(df, melt_date):\n",
    "    date = pd.to_datetime(melt_date).date()\n",
    "    df['date'] = date\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggregated/pooled values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from typing import Tuple\n",
    "from scipy.stats import mode\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "def get_window(image: np.ndarray, window_size: int, center: Tuple[int, int]) -> np.ndarray:\n",
    "    top = max(center[0] - window_size // 2, 0)\n",
    "    bottom = min(center[0] + window_size // 2 + 1, image.shape[0])\n",
    "    left = max(center[1] - window_size // 2, 0)\n",
    "    right = min(center[1] + window_size // 2 + 1, image.shape[1])\n",
    "    window = image[top:bottom, left:right]\n",
    "    return window\n",
    "\n",
    "\n",
    "# need to fix? : only calculate if the middle value is not nan - else all nan columns around 1 and 0 are going to have a value.\n",
    "\n",
    "def convolve(image, window_size, convolution_fn: Union['mean', 'min', 'max', 'sum']):\n",
    "    image = image[0].values\n",
    "    image[image == -1] = np.nan\n",
    "    \n",
    "    if convolution_fn == 'mean':\n",
    "        kernel = np.ones((window_size, window_size))  # kernel for mean convolution\n",
    "        result = np.zeros_like(image, dtype=np.float64)\n",
    "        # Compute the sum and count of non-NaN values in the kernel window\n",
    "        counts = convolve2d(~np.isnan(image), kernel, mode='same', boundary='fill', fillvalue=0)\n",
    "        sums = convolve2d(np.nan_to_num(image), kernel, mode='same', boundary='fill', fillvalue=0)\n",
    "        # Calculate the mean, ignoring NaN values\n",
    "        result[counts > 0] = sums[counts > 0] / counts[counts > 0]\n",
    "        # Set the output to NaN where all values in the kernel window are NaN\n",
    "        result[counts == 0] = np.nan\n",
    "        return result\n",
    "        \n",
    "    elif convolution_fn == 'max':\n",
    "        result = np.zeros_like(image, dtype=np.float64)\n",
    "        for i in range(image.shape[0]):\n",
    "            for j in range(image.shape[1]):\n",
    "                window = get_window(image, window_size, (i, j))\n",
    "                non_nan_values = window[~np.isnan(window)]\n",
    "                if len(non_nan_values) == 0:\n",
    "                    result[i, j] = np.nan\n",
    "                else:\n",
    "                    result[i, j] = np.nanmax(non_nan_values)\n",
    "\n",
    "    elif convolution_fn == 'min':\n",
    "        result = np.zeros_like(image, dtype=np.float64)\n",
    "        for i in range(image.shape[0]):\n",
    "            for j in range(image.shape[1]):\n",
    "                window = get_window(image, window_size, (i, j))\n",
    "                non_nan_values = window[~np.isnan(window)]\n",
    "                if len(non_nan_values) == 0:\n",
    "                    result[i, j] = np.nan\n",
    "                else:\n",
    "                    result[i, j] = np.nanmin(non_nan_values)\n",
    "        return result\n",
    "\n",
    "    elif convolution_fn == 'sum':\n",
    "        result = np.zeros_like(image, dtype=np.float64)\n",
    "        for i in range(image.shape[0]):\n",
    "            for j in range(image.shape[1]):\n",
    "                window = get_window(image, window_size, (i, j))\n",
    "                non_nan_values = window[~np.isnan(window)]\n",
    "                if len(non_nan_values) == 0:\n",
    "                    result[i, j] = np.nan\n",
    "                else:\n",
    "                    result[i, j] = np.nansum(non_nan_values)\n",
    "        return result\n",
    "        \n",
    "    else: \n",
    "        print('not available function')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Elevation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elevation(data):\n",
    "    df = data.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "    df = df[['x', 'y', 'band_data']]\n",
    "    df.rename({'band_data': 'elevation_data'}, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slope"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slope is given as degree of incline angle: 0 means flat (no slope == horizontal), 90 means (most possible slope == vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope(data):\n",
    "    slope = gg.raster.calculate_slope(data)\n",
    "    nrows, ncols = slope.shape\n",
    "    # create an array of x and y positions\n",
    "    x = np.tile(np.arange(ncols), nrows)\n",
    "    y = np.repeat(np.arange(nrows), ncols)\n",
    "    # create a DataFrame with x, y, and pixel values as columns\n",
    "    df_slope = pd.DataFrame({'col': x, 'row': y, 'slope_data': slope.flatten()})\n",
    "    return df_slope"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aspect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aspect is given as cosine radian: 0 and 360 degree = 1, 180 degree = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect(data):\n",
    "    aspect = gg.raster.calculate_aspect(data)\n",
    "    nrows, ncols = aspect.shape\n",
    "    # create an array of x and y positions\n",
    "    x = np.tile(np.arange(ncols), nrows)\n",
    "    y = np.repeat(np.arange(nrows), ncols)\n",
    "    # create a DataFrame with x, y, and pixel values as columns\n",
    "    df_aspect = pd.DataFrame({'col': x, 'row': y, 'aspect_data': aspect.flatten()})\n",
    "    df_aspect[\"aspect_data\"] = np.cos(df_aspect[\"aspect_data\"] * np.pi / 180.)\n",
    "    return df_aspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance from margin/shore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_margin():\n",
    "    data_microwave = xarray.open_dataarray(mw_path + '2019-06-08_mw.tif') # any microwave file\n",
    "    mw_val_masked = data_microwave[0].values\n",
    "    mw_val_masked = np.copy(mw_val_masked)\n",
    "    mw_val_masked[mw_val_masked==1]=0\n",
    "    dist_in_pixels = scipy.ndimage.morphology.distance_transform_edt(mw_val_masked==0, return_distances= True)\n",
    "    return dist_in_pixels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Array to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_df(convolution_raster, column_name):\n",
    "    nrows, ncols = convolution_raster.shape\n",
    "    # create an array of x and y positions\n",
    "    x = np.tile(np.arange(ncols), nrows)\n",
    "    y = np.repeat(np.arange(nrows), ncols)\n",
    "    # create a DataFrame with x, y, and pixel values as columns\n",
    "    df = pd.DataFrame({'col': x, 'row': y, column_name: convolution_raster.flatten()})\n",
    "    return df "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hours of Daylight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hours_of_daylight(df, melt_date):\n",
    "    date = pd.to_datetime(melt_date)\n",
    "    day = date.dayofyear\n",
    "    month = date.month\n",
    "    # read daylight data for respective month\n",
    "    daylight = pd.read_parquet(path_daylight + f'hours_of_daylight_month{month}.parquet')\n",
    "    # only keep column for respective day of year\n",
    "    daylight = daylight[[\"x\", \"y\", f'daylight_day_{day}']]\n",
    "    # rename column \"daylight_day_XXX\" to \"hours_of_daylight\"\n",
    "    daylight.rename({f'daylight_day_{day}': 'hours_of_daylight'}, axis=1, inplace=True)\n",
    "    return daylight\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal Information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note for testing: requires feature-engineered files of 7 days prior to testing date as input*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_information(df, melt_date):\n",
    "\n",
    "    df_temporal = df[[\"x\", \"y\"]].copy()\n",
    "\n",
    "    today = pd.to_datetime(melt_date)\n",
    "    yesterday = (today - dt.timedelta(days=1)).date()\n",
    "    file_path_yesterday = f\"{out_path}melt_{yesterday}_extended.parquet.gzip\"\n",
    "\n",
    "    if os.path.isfile(file_path_yesterday):\n",
    "        df_yesteday = pd.read_parquet(file_path_yesterday)\n",
    "        df_temporal[\"mw_yesterday\"] = df_yesteday[\"mw_value\"]\n",
    "        # df_temporal = df_temporal.join(df_yesterday[[\"mw_value\", \"mean_3\", \"mean_9\", \"sum_5\"]].add_suffix(\"_yesterday\"))\n",
    "\n",
    "        last_7_days = pd.date_range((today - dt.timedelta(days=7)), yesterday).copy()\n",
    "        file_check = [os.path.isfile(f\"{out_path}melt_{f.date()}_extended.parquet.gzip\") for f in last_7_days] \n",
    "\n",
    "        if all(file_check):\n",
    "            mw_last_7_days = pd.DataFrame()\n",
    "            #mean_3_last_7_days = pd.DataFrame()\n",
    "            #mean_9_last_7_days = pd.DataFrame()\n",
    "            #sum_5_last_7_days = pd.DataFrame()\n",
    "\n",
    "            for date in last_7_days:\n",
    "                temp = pd.read_parquet(f\"{out_path}melt_{date.date()}_extended.parquet.gzip\")\n",
    "                mw_last_7_days = pd.concat([mw_last_7_days, temp[\"mw_value\"]], axis=1)\n",
    "                #mean_3_last_7_days = pd.concat([mean_3_last_7_days, temp[\"mean_3\"]], axis=1)\n",
    "                #mean_9_last_7_days = pd.concat([mean_9_last_7_days, temp[\"mean_9\"]], axis=1)\n",
    "                #sum_5_last_7_days = pd.concat([sum_5_last_7_days, temp[\"sum_5\"]], axis=1)\n",
    "\n",
    "            df_temporal = df_temporal.assign(\n",
    "                mw_value_7_day_average=mw_last_7_days.mean(axis=1),\n",
    "                #mean_3_7_day_average=mean_3_last_7_days.mean(axis=1),\n",
    "                #mean_9_7_day_average=mean_9_last_7_days.mean(axis=1),\n",
    "                #sum_5_7_day_average=sum_5_last_7_days.mean(axis=1),\n",
    "            )\n",
    "        else:\n",
    "            df_temporal = df_temporal.assign(\n",
    "                mw_value_7_day_average=df[\"mw_value\"],\n",
    "                #mean_3_7_day_average=np.nan,\n",
    "                #mean_9_7_day_average=np.nan,\n",
    "                #sum_5_7_day_average=np.nan\n",
    "            )\n",
    "        return df_temporal\n",
    "    else:\n",
    "        nan_df = pd.DataFrame(columns=[\"x\", \"y\", \"mw_value_yesterday\", \"mw_value_7_day_average\"]) # \"mean_3_yesterday\", \"mean_3_7_day_average\", \"mean_9_yesterday\", \"mean_9_7_day_average\", \"sum_3_yesterday\", \"sum_3_7_day_average\"], index=df.index)\n",
    "        nan_df[\"x\"] = df[\"x\"]\n",
    "        nan_df[\"y\"] = df[\"y\"]\n",
    "        nan_df['mw_value_yesterday'] = df['mw_value']\n",
    "        nan_df['mw_value_7_day_average'] = df['mw_value']\n",
    "\n",
    "        return nan_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(mw_path, path_dataframe_plain):\n",
    "    # get plain files:\n",
    "    df_plain_files = [f for f in listdir(path_dataframe_plain) if isfile(join(path_dataframe_plain, f))]\n",
    "    # microwave files:\n",
    "    mw_files = [f for f in listdir(mw_path) if isfile(join(mw_path, f))]\n",
    "    return  mw_files, df_plain_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(mw_files_list, df_plain_files_list, path_elevation, out_path, write = False):\n",
    "    # get plain files:\n",
    "    df_plain_files = df_plain_files_list\n",
    "    # microwave files:\n",
    "    mw_files = mw_files_list\n",
    "    # load elevation data:\n",
    "    data_elevation_xarray = xarray.open_dataarray(path_elevation)\n",
    "    data_elevation_rasterio = rasterio.open(path_elevation)\n",
    "    # calculate distance to margin:\n",
    "    distance_margin = distance_to_margin()\n",
    "\n",
    "    for df_file in df_plain_files:\n",
    "        melt_date =  df_file[5:15]\n",
    "        print(melt_date)\n",
    "        for mw_file in mw_files:\n",
    "            if mw_file.startswith(melt_date):\n",
    "                data_mw = xarray.open_dataarray(mw_path + mw_file)\n",
    "                df = pd.read_parquet(path_dataframe_plain + df_file)\n",
    "                # cap outliers above 5 in opt_value column to 5\n",
    "                df.loc[df['opt_value'] > 5, 'opt_value'] = 5\n",
    "                # add row and column features:\n",
    "                df = add_row_and_col(df)\n",
    "                # remove water in mw:\n",
    "                df = df.loc[df['mw_value'] != -1]\n",
    "                # get convolutions:\n",
    "                df_conv_mean_3 = array_to_df(convolve(data_mw, 3, 'mean'), 'mean_3')\n",
    "                df_conv_mean_9 = array_to_df(convolve(data_mw, 9, 'mean'), 'mean_9')\n",
    "                df_conv_sum_5 = array_to_df(convolve(data_mw, 5, 'sum'), 'sum_5')\n",
    "                # merge convolution:\n",
    "                df_combined = pd.merge(df, df_conv_mean_3, how = 'left', on = ['row', 'col'])\n",
    "                df_combined = pd.merge(df_combined, df_conv_mean_9, how = 'left', on = ['row', 'col'])\n",
    "                df_combined = pd.merge(df_combined, df_conv_sum_5, how = 'left', on = ['row', 'col'])\n",
    "                # add date:\n",
    "                df = add_date(df_combined, melt_date)\n",
    "                # add temporal information:\n",
    "                df_temporal = add_temporal_information(df, melt_date)\n",
    "                df = pd.merge(df, df_temporal, how = 'left', on = [\"y\",'x'])\n",
    "                # add and merge daylight_data:\n",
    "                df_daylight = add_hours_of_daylight(df, melt_date)\n",
    "                df = pd.merge(df, df_daylight, how = 'left', on = [\"y\",'x'])\n",
    "                # add and merge elevation data:\n",
    "                df_elevation = add_elevation(data_elevation_xarray)\n",
    "                df = pd.merge(df, df_elevation, how = 'left', on = ['y', 'x'])\n",
    "                # get and merge slope data:\n",
    "                df_slope = get_slope(data_elevation_rasterio)\n",
    "                df = pd.merge(df, df_slope[[\"slope_data\"]], how=\"left\", right_index=True, left_index=True)\n",
    "                # get and merge aspect data:\n",
    "                df_aspect = get_aspect(data_elevation_rasterio)\n",
    "                df = pd.merge(df, df_aspect[[\"aspect_data\"]], how=\"left\", right_index=True, left_index=True) \n",
    "                # add and merge distance to margin data:\n",
    "                df_distance = array_to_df(distance_margin, 'distance_to_margin')\n",
    "                df = pd.merge(df, df_distance, how = 'left', on = ['row', 'col'])\n",
    "                # write to parquet:\n",
    "                if write == True:\n",
    "                    df.to_parquet(out_path + 'melt_'+ melt_date + '_extended.parquet.gzip', index= False)                    \n",
    "    return df\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main(mw_path, path_dataframe_plain, path_elevation, out_path)\n",
    "main(*get_files(mw_path, path_dataframe_plain), path_elevation, out_path)# , write = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1951/1152928399.py:6: DeprecationWarning: Please use `distance_transform_edt` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  dist_in_pixels = scipy.ndimage.morphology.distance_transform_edt(mw_val_masked==0, return_distances= True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-12\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't infer object conversion type: 0    2019-06-12\n1    2019-06-12\n2    2019-06-12\n3    2019-06-12\n4    2019-06-12\n5    2019-06-12\n6    2019-06-12\n7    2019-06-12\n8    2019-06-12\n9    2019-06-12\nName: date, dtype: object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/Scripts/04_feature_engineering.ipynb Cell 36\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/Scripts/04_feature_engineering.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m main([\u001b[39m'\u001b[39m\u001b[39m2019-06-12_mw.tif\u001b[39m\u001b[39m'\u001b[39m], [\u001b[39m'\u001b[39m\u001b[39mmelt_2019-06-12.parquet.gzip\u001b[39m\u001b[39m'\u001b[39m], path_elevation, out_path, write\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/Scripts/04_feature_engineering.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m df\n",
      "\u001b[1;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/Scripts/04_feature_engineering.ipynb Cell 36\u001b[0m in \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/Scripts/04_feature_engineering.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m             \u001b[39m# write to parquet:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/Scripts/04_feature_engineering.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m             \u001b[39mif\u001b[39;00m write \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/Scripts/04_feature_engineering.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m                 df\u001b[39m.\u001b[39;49mto_parquet(out_path \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mmelt_\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m melt_date \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m_extended.parquet.gzip\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)                    \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/Scripts/04_feature_engineering.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/.venv/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/.venv/lib/python3.9/site-packages/pandas/core/frame.py:2975\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2888\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2889\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2890\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2972\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2973\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2975\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[1;32m   2976\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2977\u001b[0m     path,\n\u001b[1;32m   2978\u001b[0m     engine,\n\u001b[1;32m   2979\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2980\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   2981\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m   2982\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2983\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2984\u001b[0m )\n",
      "File \u001b[0;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/.venv/lib/python3.9/site-packages/pandas/io/parquet.py:428\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m    426\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m--> 428\u001b[0m impl\u001b[39m.\u001b[39;49mwrite(\n\u001b[1;32m    429\u001b[0m     df,\n\u001b[1;32m    430\u001b[0m     path_or_buf,\n\u001b[1;32m    431\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    432\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    433\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m    434\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    435\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    436\u001b[0m )\n\u001b[1;32m    438\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, io\u001b[39m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/.venv/lib/python3.9/site-packages/pandas/io/parquet.py:310\u001b[0m, in \u001b[0;36mFastParquetImpl.write\u001b[0;34m(self, df, path, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstorage_options passed with file object or non-fsspec file path\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    309\u001b[0m \u001b[39mwith\u001b[39;00m catch_warnings(record\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mwrite(\n\u001b[1;32m    311\u001b[0m         path,\n\u001b[1;32m    312\u001b[0m         df,\n\u001b[1;32m    313\u001b[0m         compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    314\u001b[0m         write_index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    315\u001b[0m         partition_on\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m    316\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    317\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/.venv/lib/python3.9/site-packages/fastparquet/writer.py:1214\u001b[0m, in \u001b[0;36mwrite\u001b[0;34m(filename, data, row_group_offsets, compression, file_scheme, open_with, mkdirs, has_nulls, write_index, partition_on, fixed_text, append, object_encoding, times, custom_metadata, stats)\u001b[0m\n\u001b[1;32m   1211\u001b[0m check_column_names(data\u001b[39m.\u001b[39mcolumns, partition_on, fixed_text,\n\u001b[1;32m   1212\u001b[0m                    object_encoding, has_nulls)\n\u001b[1;32m   1213\u001b[0m ignore \u001b[39m=\u001b[39m partition_on \u001b[39mif\u001b[39;00m file_scheme \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msimple\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m []\n\u001b[0;32m-> 1214\u001b[0m fmd \u001b[39m=\u001b[39m make_metadata(data, has_nulls\u001b[39m=\u001b[39;49mhas_nulls, ignore_columns\u001b[39m=\u001b[39;49mignore,\n\u001b[1;32m   1215\u001b[0m                     fixed_text\u001b[39m=\u001b[39;49mfixed_text,\n\u001b[1;32m   1216\u001b[0m                     object_encoding\u001b[39m=\u001b[39;49mobject_encoding,\n\u001b[1;32m   1217\u001b[0m                     times\u001b[39m=\u001b[39;49mtimes, index_cols\u001b[39m=\u001b[39;49mindex_cols,\n\u001b[1;32m   1218\u001b[0m                     partition_cols\u001b[39m=\u001b[39;49mpartition_on)\n\u001b[1;32m   1219\u001b[0m \u001b[39mif\u001b[39;00m custom_metadata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1220\u001b[0m     kvm \u001b[39m=\u001b[39m fmd\u001b[39m.\u001b[39mkey_value_metadata \u001b[39mor\u001b[39;00m []\n",
      "File \u001b[0;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/.venv/lib/python3.9/site-packages/fastparquet/writer.py:824\u001b[0m, in \u001b[0;36mmake_metadata\u001b[0;34m(data, has_nulls, ignore_columns, fixed_text, object_encoding, times, index_cols, partition_cols)\u001b[0m\n\u001b[1;32m    822\u001b[0m     se\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m column\n\u001b[1;32m    823\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     se, \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m find_type(data[column], fixed_text\u001b[39m=\u001b[39;49mfixed,\n\u001b[1;32m    825\u001b[0m                          object_encoding\u001b[39m=\u001b[39;49moencoding, times\u001b[39m=\u001b[39;49mtimes,\n\u001b[1;32m    826\u001b[0m                          is_index\u001b[39m=\u001b[39;49mis_index)\n\u001b[1;32m    827\u001b[0m col_has_nulls \u001b[39m=\u001b[39m has_nulls\n\u001b[1;32m    828\u001b[0m \u001b[39mif\u001b[39;00m has_nulls \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/.venv/lib/python3.9/site-packages/fastparquet/writer.py:125\u001b[0m, in \u001b[0;36mfind_type\u001b[0;34m(data, fixed_text, object_encoding, times, is_index)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39melif\u001b[39;00m dtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mO\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m object_encoding \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39minfer\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m         object_encoding \u001b[39m=\u001b[39m infer_object_encoding(data)\n\u001b[1;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m object_encoding \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    128\u001b[0m         \u001b[39mtype\u001b[39m, converted_type, width \u001b[39m=\u001b[39m (parquet_thrift\u001b[39m.\u001b[39mType\u001b[39m.\u001b[39mBYTE_ARRAY,\n\u001b[1;32m    129\u001b[0m                                        parquet_thrift\u001b[39m.\u001b[39mConvertedType\u001b[39m.\u001b[39mUTF8,\n\u001b[1;32m    130\u001b[0m                                        \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/c/Users/linas/OneDrive/Documents/ITU/Semester3/RP_Greenland/RP_Greenland/.venv/lib/python3.9/site-packages/fastparquet/writer.py:342\u001b[0m, in \u001b[0;36minfer_object_encoding\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    341\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 342\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt infer object conversion type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m head)\n",
      "\u001b[0;31mValueError\u001b[0m: Can't infer object conversion type: 0    2019-06-12\n1    2019-06-12\n2    2019-06-12\n3    2019-06-12\n4    2019-06-12\n5    2019-06-12\n6    2019-06-12\n7    2019-06-12\n8    2019-06-12\n9    2019-06-12\nName: date, dtype: object"
     ]
    }
   ],
   "source": [
    "df = main(['2019-06-12_mw.tif'], ['melt_2019-06-12.parquet.gzip'], path_elevation, out_path, write= True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for testing values around the 0-1 change in the data:\n",
    "\n",
    "# tt = data_mw.values\n",
    "# # indices = np.where(tt == 1)\n",
    "# tt[0][74:80, 622:628]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe81b6f13d6b0ad05b54a8d717c7eaa3743b1b1bf0e5cb2d6ec80103feaa1c84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
